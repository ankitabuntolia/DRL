{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_StableBaselines3_Demo.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankitabuntolia/DRL/blob/main/02_StableBaselines3_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Deep Reinforcement Learning - Stable Baselines3 Demo\n",
        "\n",
        "Github Repo: [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)\n",
        "\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines3.\n",
        "\n",
        "It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
        "\n",
        "Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)\n",
        "\n",
        "Pybullet source code: https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf67YKqAFzjH"
      },
      "source": [
        "## Install Dependencies and Stable-Baselines3 Using Pip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "source": [
        "!apt update\n",
        "!apt-get install -y xvfb x11-utils ffmpeg\n",
        "!pip install gym pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*\n",
        "!pip install stable-baselines3[extra] pybullet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Import policy, RL agent, Wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "source": [
        "import os, shutil\n",
        "import glob, io, base64\n",
        "\n",
        "import pybullet_envs\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.vec_env import VecNormalize, VecVideoRecorder\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "# Plotting and notebook imports\n",
        "from IPython.display import HTML, clear_output\n",
        "from IPython import display\n",
        "\n",
        "# start virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "pydisplay = Display(visible=0, size=(640, 480))\n",
        "pydisplay.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hhAQuCKA6ue"
      },
      "source": [
        "## Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0Stn4OCBAuM"
      },
      "source": [
        "def concatenate_videos(video_dir):\n",
        "    \"\"\"\n",
        "    Merge all mp4 videos in video_dir.\n",
        "    \"\"\"\n",
        "    outfile = os.path.join(video_dir, 'merged_video.mp4')\n",
        "    cmd = \"ffmpeg -i \\\"concat:\"\n",
        "    mp4list = glob.glob(os.path.join(video_dir, '*.mp4'))\n",
        "    tmpfiles = []\n",
        "    # build ffmpeg command and create temp files\n",
        "    for f in mp4list:\n",
        "        file = os.path.join(video_dir, \"temp\" + str(mp4list.index(f) + 1) + \".ts\")\n",
        "        os.system(\"ffmpeg -i \" + f + \" -c copy -bsf:v h264_mp4toannexb -f mpegts \" + file)\n",
        "        tmpfiles.append(file)\n",
        "    for f in tmpfiles:\n",
        "        cmd += f\n",
        "        if tmpfiles.index(f) != len(tmpfiles)-1:\n",
        "            cmd += \"|\"\n",
        "        else:\n",
        "            cmd += f\"\\\" -c copy  -bsf:a aac_adtstoasc {outfile}\"\n",
        "    # execute ffmpeg command to combine videos\n",
        "    os.system(cmd)\n",
        "    # cleanup\n",
        "    for f in tmpfiles + mp4list:\n",
        "        if f != outfile:\n",
        "            os.remove(f)\n",
        "    # --\n",
        "    return outfile\n",
        "\n",
        "def show_video(video_dir):\n",
        "    \"\"\"\n",
        "    Show video in the output of a code cell.\n",
        "    \"\"\"\n",
        "    # merge all videos\n",
        "    mp4 = concatenate_videos(video_dir)    \n",
        "    if mp4:\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Could not find video\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q52-O38xBkYa"
      },
      "source": [
        "## Define global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AlLkze6Bjou"
      },
      "source": [
        "env_id = \"HalfCheetahBulletEnv-v0\"\n",
        "n_envs = 1\n",
        "video_length = 500\n",
        "log_dir = \"logs/\"\n",
        "video_folder = f\"{log_dir}videos/\"\n",
        "model_path = os.path.join(log_dir, \"ppo_halfcheetah\")\n",
        "stats_path = os.path.join(log_dir, \"vec_normalize.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c8VHsiXC7dL"
      },
      "source": [
        "## Create and wrap the environments\n",
        "\n",
        "Normalizing input features may be essential to successful training of an RL agent (by default, images are scaled but not other types of input), for instance when training on [PyBullet](https://github.com/bulletphysics/bullet3/) environments. For that, the `VecNormalize` exists, and will compute a running average and standard deviation of input features (it can do the same for rewards).\n",
        "\n",
        "More information about `VecNormalize`:\n",
        "- [Documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#stable_baselines3.common.vec_env.VecNormalize)\n",
        "- [Discussion](https://github.com/hill-a/stable-baselines/issues/698)\n",
        "\n",
        "---\n",
        "\n",
        "To observe the agent behavior during environment rollouts one can simply record the frames and interactions by applying the `VecVideoRecorder` envrionment wrapper.\n",
        "\n",
        "More information about `VecVideoRecorder`:\n",
        "- [Documentation](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#record-a-video)\n",
        "\n",
        "To learn more about vectorized environments follow this link:\n",
        "- [Documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmxIq5UeC3Nj"
      },
      "source": [
        "env = make_vec_env(env_id, n_envs)\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n",
        "env = VecVideoRecorder(env, video_folder,\n",
        "                       record_video_trigger=lambda x: x == 0, video_length=video_length,\n",
        "                       name_prefix=\"random-agent-{}\".format(env_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxUMGsl5mabF"
      },
      "source": [
        "### Train the agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7stOpVs2p80"
      },
      "source": [
        "For training Stable-Baselines3 offers a set of the most commonly used baseline methods in reinforcement learning:\n",
        "\n",
        "* A2C ([Asynchronous Actor-Critic](https://arxiv.org/abs/1602.01783))\n",
        "* DDPG ([Deep Deterministic Policy Gradient](https://arxiv.org/abs/1509.02971))\n",
        "* DQN ([Deep Q-Networks](https://arxiv.org/abs/1312.5602))\n",
        "* HER ([Hindsight Experience Replay](https://arxiv.org/abs/1707.01495))\n",
        "* PPO ([Proximal Policy Optimization](https://arxiv.org/abs/1707.06347))\n",
        "* SAC ([Soft Actor-Critic](https://arxiv.org/abs/1801.01290))\n",
        "* TD3 ([Twin Delayed Deep Deterministic policy gradient](https://arxiv.org/abs/1802.09477))\n",
        "\n",
        "For a more detailed description follow the documentation:\n",
        "* [Documentation](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html)\n",
        "\n",
        "As a neural network architecture we are simply using a Multilayer Perceptron. Stable-Baselines3 currently offers two types:\n",
        "* MlpPolicy\n",
        "* CnnPolicy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQmsSZUHKNRG"
      },
      "source": [
        "model = PPO('MlpPolicy', env, verbose=True)\n",
        "model.learn(total_timesteps=2000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZYBIVoLmcR4"
      },
      "source": [
        "### Save the agent and the normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpMDXP0vmezv"
      },
      "source": [
        "# Save model and the normalized statistics\n",
        "model.save(model_path)\n",
        "env.save(stats_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eezphIrRmr-Y"
      },
      "source": [
        "### Test model: load the saved agent and normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQT1k7lWmmTL"
      },
      "source": [
        "# Load the agent\n",
        "model = PPO.load(model_path)\n",
        "\n",
        "# Load the saved statistics\n",
        "env = make_vec_env(env_id, n_envs=n_envs)\n",
        "env = VecNormalize.load(stats_path, env)\n",
        "#  do not update them at test time\n",
        "env.training = False\n",
        "# reward normalization is not needed at test time\n",
        "env.norm_reward = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zC2IMJUm3Kw"
      },
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, env)\n",
        "print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geIw9s-5CbdH"
      },
      "source": [
        "## Show video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AiagLRuCe0x"
      },
      "source": [
        "show_video(video_folder)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}